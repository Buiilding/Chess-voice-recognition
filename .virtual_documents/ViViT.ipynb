





pip install opencv-python tensorflow mediapipe



import os
import cv2
import numpy as np
import tensorflow as tf
from tensorflow import keras
from PIL import Image
from tensorflow.keras import layers
import matplotlib.pyplot as plt





# SYSTEM
PATH_TO_PROJECT = "C:/Users/peter/Github/Chess-voice-recognition"

# DATA
BATCH_SIZE = 2
AUTO = tf.data.AUTOTUNE
INPUT_size = (30, 112, 112, 3) 
NUM_CLASSES = 25

# OPTIMIZER
LEARNING_RATE = 1e-4
WEIGHT_DECAY = 1e-5

# TRAINING
EPOCHS = 100

# TUBELET EMBEDDING
PATCH_SIZE = (8, 8, 8)
NUM_PATCHES = (INPUT_size[0] // PATCH_SIZE[0]) ** 2

# ViViT ARCHITECTURE
LAYER_NORM_EPS = 1e-6
PROJECTION_DIM = 128
NUM_HEADS = 8
NUM_LAYERS = 8








import os  # Ensure os is imported
import numpy as np
from sklearn.model_selection import train_test_split

dataset_dir = "Data_2/White"
dataset_dir = os.path.join(PATH_TO_PROJECT, dataset_dir)

# Lists to store the data and labels
data = []
labels = []

# List all class folders in the new dataset directory
class_folders = os.listdir(dataset_dir)

# Loop through each class folder
for class_folder in class_folders:
    class_dir = os.path.join(dataset_dir, class_folder)
    
    files = [file for file in os.listdir(class_dir) if file.endswith(".png")]
    
    for file in files: 
        data.append(os.path.join(class_dir, file))
        labels.append(class_folder)

# Split the data into training, validation, and test sets
train_data, temp_data, train_labels, temp_labels = train_test_split(data, labels, test_size=0.3, random_state=42)
valid_data, test_data, valid_labels, test_labels = train_test_split(temp_data, temp_labels, test_size=0.5, random_state=42)



print("Train data")
print(f"Train data size: {len(train_data)}")
print(f"Train labels size: {len(train_labels)}")
print("\n")

print("Validation data")
print(f"Validation data size: {len(valid_data)}")
print(f"Validation labels size: {len(valid_labels)}")
print("\n")

print("Test data")
print(f"Test data size: {len(test_data)}")
print(f"Test labels size: {len(test_labels)}")






@tf.function
def preprocess(frames: tf.Tensor, label: tf.Tensor):
    """Preprocess the frames tensors and parse the labels."""
    # Preprocess images
    frames = tf.image.convert_image_dtype(
        frames[
            ..., tf.newaxis
        ],  # The new axis is to help for further processing with Conv3D layers
        tf.float32,
    )
    # Parse label
    label = tf.cast(label, tf.float16)
    return frames, label


def prepare_dataloader(
    videos: np.ndarray,
    labels: np.ndarray,
    loader_type: str = "train",
    batch_size: int = BATCH_SIZE,
):
    """Utility function to prepare the dataloader."""
    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))

    if loader_type == "train":
        dataset = dataset.shuffle(BATCH_SIZE * 2)

    dataloader = (
        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
        .batch(batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )
    return dataloader

trainloader = prepare_dataloader(train_data, train_labels, "train")
validloader = prepare_dataloader(valid_data, valid_labels, "valid")
testloader = prepare_dataloader(test_data, test_labels, "test")





class TubeletEmbedding(layers.Layer):
    def __init__(self, embed_dim, patch_size, **kwargs):
        super().__init__(**kwargs)
        self.projection = layers.Conv3D(
            filters=embed_dim,
            kernel_size=patch_size,
            strides=patch_size,
            padding="VALID",
        )
        self.flatten = layers.Resize(target_size=(-1, embed_dim))

    def call(self, videos):
        projected_patches = self.projection(videos)
        flattened_patches = self.flatten(projected_patches)
        return flattened_patches





class PositionalEncoder(layers.Layer):
    def __init__(self, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim

    def build(self, input_size):
        _, num_tokens, _ = input_size
        self.position_embedding = layers.Embedding(
            input_dim=num_tokens, output_dim=self.embed_dim
        )
        self.positions = tf.range(start=0, limit=num_tokens, delta=1)

    def call(self, encoded_tokens):
        # Encode the positions and add it to the encoded tokens
        encoded_positions = self.position_embedding(self.positions)
        encoded_tokens = encoded_tokens + encoded_positions
        return encoded_tokens








# def create_vivit_classifier(
#     tubelet_embedder,
#     positional_encoder,
#     input_size=INPUT_size,
#     transformer_layers=NUM_LAYERS,
#     num_heads=NUM_HEADS,
#     embed_dim=PROJECTION_DIM,
#     layer_norm_eps=LAYER_NORM_EPS,
#     num_classes=NUM_CLASSES,
# 
# ):
#     # Get the input layer
#     inputs = layers.Input(size=input_size)
#     # Create patches.
#     patches = tubelet_embedder(inputs)
#     # Encode patches.
#     encoded_patches = positional_encoder(patches)
# 
#     # Create multiple layers of the Transformer block.
#     for _ in range(transformer_layers):
#         # Layer normalization and MHSA
#         x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
#         attention_output = layers.MultiHeadAttention(
#             num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1
#         )(x1, x1)
# 
#         # Skip connection
#         x2 = layers.Add()([attention_output, encoded_patches])
# 
#         # Layer Normalization and MLP
#         x3 = layers.LayerNormalization(epsilon=1e-6)(x2)
#         x3 = keras.Sequential(
#             [
#                 layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),
#                 layers.Dense(units=embed_dim, activation=tf.nn.gelu),
#             ]
#         )(x3)
# 
#         # Skip connection
#         encoded_patches = layers.Add()([x3, x2])
# 
#     # Layer normalization and Global average pooling.
#     representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)
#     representation = layers.GlobalAvgPool1D()(representation)
# 
#     # Classify outputs.
#     outputs = layers.Dense(units=num_classes, activation="softmax")(representation)
# 
#     # Create the Keras model.
#     model = keras.Model(inputs=inputs, outputs=outputs)
#     return model

from tensorflow.keras import layers, regularizers

def create_vivit_classifier(
    tubelet_embedder,
    positional_encoder,
    input_size=INPUT_size,
    transformer_layers=NUM_LAYERS,
    num_heads=NUM_HEADS,
    embed_dim=PROJECTION_DIM,
    layer_norm_eps=LAYER_NORM_EPS,
    num_classes=NUM_CLASSES,
    dropout_rate=0.2,  # Adjust the dropout rate as needed
    l2_regularization=0.01,  # Adjust the regularization strength as needed
):
    # Get the input layer
    inputs = layers.Input(size=input_size)
    # Create patches.
    patches = tubelet_embedder(inputs)
    # Encode patches.
    encoded_patches = positional_encoder(patches)

    # Create multiple layers of the Transformer block.
    for _ in range(transformer_layers):
        # Layer normalization and MHSA
        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
        attention_output = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1
        )(x1, x1)

        # Skip connection
        x2 = layers.Add()([attention_output, encoded_patches])

        # Layer Normalization and MLP with L2 regularization
        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)
        x3 = layers.Dense(
            units=embed_dim * 4,
            activation=tf.nn.gelu,
            kernel_regularizer=regularizers.l2(l2_regularization),
        )(x3)
        x3 = layers.Dropout(dropout_rate)(x3)  # Applying Dropout

        x3 = layers.Dense(
            units=embed_dim,
            activation=tf.nn.gelu,
            kernel_regularizer=regularizers.l2(l2_regularization),
        )(x3)
        x3 = layers.Dropout(dropout_rate)(x3)  # Applying Dropout

        # Skip connection
        encoded_patches = layers.Add()([x3, x2])

    # Layer normalization and Global average pooling.
    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)
    representation = layers.GlobalAvgPool1D()(representation)

    # Classify outputs.
    outputs = layers.Dense(units=num_classes, activation="softmax")(representation)

    # Create the Keras model.
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model





# Initialize model
model = create_vivit_classifier(
    tubelet_embedder=TubeletEmbedding(
        embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE
    ),
    positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),
)

# Compile the model with the optimizer, loss function
# and the metrics. 
# Tested [Adam] - RMSdrop - Nadam - Adamax
optimizer = keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)
model.compile(
    optimizer=optimizer,
    loss="sparse_categorical_crossentropy",
    metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
    ],
)

# Train the model.
history = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)


model.evaluate(test_data, test_labels)





model.save('Saved model/ViViT.h5')





acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']


# The accuracy
plt.figure(figsize=(16, 16))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.grid()
plt.title('Training and Validation Accuracy')

# The loss
plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Loss')
plt.ylim([0, max(plt.ylim())])
plt.grid()
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()
