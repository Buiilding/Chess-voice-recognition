{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1685893365918,
     "user": {
      "displayName": "User Buyer",
      "userId": "09440350223830875445"
     },
     "user_tz": -420
    },
    "id": "0gaIPQZTyR0Q",
    "outputId": "927e9672-b86e-4bd8-b1d4-0e1d66d5e43a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun  4 15:42:44 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#check GPU device\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18539,
     "status": "ok",
     "timestamp": 1685932523706,
     "user": {
      "displayName": "User Buyer",
      "userId": "09440350223830875445"
     },
     "user_tz": -420
    },
    "id": "a5OICdiE28j_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1685893386143,
     "user": {
      "displayName": "User Buyer",
      "userId": "09440350223830875445"
     },
     "user_tz": -420
    },
    "id": "zzWtaYy-3CM-",
    "outputId": "db587cda-2225-4d5e-f103-a903c9c27317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/1qwXu3FImYcrgP2McJlInlWLrHVHgUa3g/SLC_Project/datasets/500data\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/SLC_Project/datasets/500data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Dc3FDWAlRLES"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0NcjLLZCRE6q"
   },
   "outputs": [],
   "source": [
    "train_path = 'C:/Users/tuana/SLC_PRO/500data_resized/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-KUkCFffRcW0"
   },
   "outputs": [],
   "source": [
    "folders = os.listdir(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14048,
     "status": "ok",
     "timestamp": 1685893409935,
     "user": {
      "displayName": "User Buyer",
      "userId": "09440350223830875445"
     },
     "user_tz": -420
    },
    "id": "9Ls6pnl0Q6sL",
    "outputId": "6cada3ae-8f74-43f5-cd71-554fa85c3841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\0\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\1\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\2\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\3\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\4\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\5\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\6\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\7\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\8\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\9\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\a\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\b\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\bye\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\c\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\d\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\del\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\e\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\F\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\G\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\good\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\good morning\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\H\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\hello\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\I\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\J\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\K\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\L\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\little bit\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\M\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\N\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\no\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\nothing\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\NULL\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\O\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\P\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\pardon\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\please\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\project\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\Q\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\R\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\S\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\space\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\T\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\U\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\V\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\W\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\whats up\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\X\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\Y\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\yes\n",
      "500\n",
      "C:/Users/tuana/SLC_PRO/500data_resized/train\\Z\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "for f in folders:\n",
    "  sub_path = os.path.join(train_path,f)\n",
    "  print(sub_path)\n",
    "  ims = [x for x in os.listdir(sub_path) if x.lower().endswith('.jpg')]\n",
    "  print(len(ims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlIdq8VL_V3o"
   },
   "outputs": [],
   "source": [
    "#!rm -rf /content/drive/MyDrive/SLC_Project/datasets/500data/alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cJBv3nE5qNU"
   },
   "outputs": [],
   "source": [
    "#!unzip -q alldata.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihZYfM3N5j5A"
   },
   "outputs": [],
   "source": [
    "#!unzip -q datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1685932523706,
     "user": {
      "displayName": "User Buyer",
      "userId": "09440350223830875445"
     },
     "user_tz": -420
    },
    "id": "Zy23C-pixr4i",
    "outputId": "5a01765a-842a-4758-a1ce-4da96b25273a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuana\\SLC_PRO\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #operated by facebook with a lot of library\n",
    "import torch.nn as nn\n",
    "#import tqdm\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes = 10 ) :\n",
    "        # super(AlexNet, self). __init__()\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(3,32,kernel_size = 3 , stride = 1, padding=1),\n",
    "            nn.ReLU())\n",
    "            # nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(32,64,kernel_size=3, stride =1 , padding = 1),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.conv_3 = nn.Sequential(\n",
    "            nn.Conv2d(64,64,kernel_size = 3, stride =1 , padding = 1),\n",
    "            # nn.BatchNorm2d(384), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.conv_4 = nn.Sequential(\n",
    "            nn.Conv2d(64,128,kernel_size = 3 , stride = 1, padding =1),\n",
    "            # nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride =2 ))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 =nn. Sequential( \n",
    "            nn.Linear(8192,526),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(526,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc3= nn.Sequential(\n",
    "            nn.Linear(128, num_classes))\n",
    "        \n",
    "    def forward(self,x):\n",
    "            out = self.conv_1(x)\n",
    "            out = self.conv_2(out)\n",
    "            out = self.conv_3(out)\n",
    "            out = self.conv_4(out)\n",
    "            out = self.flatten(out)\n",
    "            # out = out.reshape(out.size(0), -1 )#flatten\n",
    "            out = self.fc1(out)\n",
    "            out = self.fc2 (out)\n",
    "            out = self.fc3(out)\n",
    "            #out = nn.Softmax(out)\n",
    "            return out\n",
    "def unit_test(b,c,h,w):\n",
    "    model = Model(num_classes = 27)\n",
    "    im_input = torch.randn((b,c,w,h))\n",
    "    out = model(im_input)\n",
    "    print(f'chick qua qua')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oYRbuL0wBocc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "model = Model(num_classes = 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685892244572,
     "user": {
      "displayName": "Bùi Đình Tuấn Anh",
      "userId": "04364186425085514423"
     },
     "user_tz": -420
    },
    "id": "yuaG0KUGC50o",
    "outputId": "0a5f1fe3-463c-4bcb-c958-7f00bc48921f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv_1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (conv_2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_3): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_4): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=8192, out_features=526, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=526, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (fc3): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=51, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "si6isghmC7B-"
   },
   "outputs": [],
   "source": [
    "#create dataloader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train_transform = transforms.Compose((\n",
    "    transforms.Resize(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),)\n",
    ")\n",
    "data_transform = transforms.Compose((\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456 , 0.406],\n",
    "                         std =[0.229, 0.224, 0.225],)\n",
    "))\n",
    "\n",
    "# Intinialize dataset based-on folder path\n",
    "train_dataset = datasets.ImageFolder(root='C:/Users/tuana/SLC_PRO/500data_resized/train', transform = train_transform)\n",
    "# BTVN\n",
    "val_dataset = datasets.ImageFolder(root = 'C:/Users/tuana/SLC_PRO/500data_resized/val', transform = data_transform) #note val doesn't need RandomHorizontalFlip\n",
    "\n",
    "# Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "# BTVN\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 64, num_workers = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HGJeG7LGOi5R"
   },
   "outputs": [],
   "source": [
    "# Setting device for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_lgJidU887br"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XACf7MxiGutP"
   },
   "outputs": [],
   "source": [
    "# Choose Loss function and Optimizer\n",
    "import torch.nn as nn\n",
    "# from torch.optim import optim\n",
    "from torch import optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4ieKyiRgXSUs"
   },
   "outputs": [],
   "source": [
    "save_best_model = 'C:/Users/tuana/SLC_PRO/Weights/best.pth'\n",
    "os.makedirs(save_best_model,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RwOySDLfSp-o"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# Evaluation metrics\n",
    "def accuracy(out, labels):\n",
    "  _, pred = torch.max(out, dim=1)\n",
    "  return torch.sum(pred==labels).item()\n",
    "def train(model, num_epochs, optimizer, criterion, train_loader, val_loader, save_path):\n",
    "    # Setting device for training\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  model = model.to(device)\n",
    "  # train the model for 10 epochs\n",
    "  best_acc = 0.0\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move the data to the device (GPU or CPU)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate training accuracy and loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        _, predicted = output.max(1)\n",
    "        train_correct += predicted.eq(target).sum().item()\n",
    "        print(f'train_loss {train_loss}')\n",
    "        print(f'train_correct {train_correct}')\n",
    "\n",
    "\n",
    "    # calculate average training accuracy and loss for the epoch\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    print(f'train_loss {train_loss}')\n",
    "    print(f'train_acc {train_acc}')\n",
    "    # evaluate the model on the test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            # move the data to the device (GPU or CPU)\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # calculate test accuracy and loss\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            _, predicted = output.max(1)\n",
    "            test_correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    # calculate average test accuracy and loss for the epoch\n",
    "    test_loss /= len(val_loader.dataset)\n",
    "    test_acc = test_correct / len(val_loader.dataset)\n",
    "    if test_acc > best_acc:\n",
    "      best_acc = test_acc\n",
    "      torch.save(model.state_dict(), save_path)\n",
    "    # print the results for the epoch\n",
    "    print('Epoch {}/{}: Train Loss: {:.6f}, Train Acc: {:.6f}, Test Loss: {:.6f}, Test Acc: {:.6f}'.format(\n",
    "        epoch+1, num_epochs, train_loss, train_acc, test_loss, test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "error",
     "timestamp": 1685869359883,
     "user": {
      "displayName": "Bùi Đình Tuấn Anh",
      "userId": "04364186425085514423"
     },
     "user_tz": -420
    },
    "id": "-isn7j0RS8jZ",
    "outputId": "4825b240-9bac-4625-b723-b5c865726780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 251.37034606933594\n",
      "train_correct 3\n",
      "train_loss 502.4737854003906\n",
      "train_correct 5\n",
      "train_loss 754.3697204589844\n",
      "train_correct 7\n",
      "train_loss 1006.523681640625\n",
      "train_correct 9\n",
      "train_loss 1258.8030090332031\n",
      "train_correct 11\n",
      "train_loss 1510.8497772216797\n",
      "train_correct 11\n",
      "train_loss 1762.8378295898438\n",
      "train_correct 13\n",
      "train_loss 2014.5082092285156\n",
      "train_correct 13\n",
      "train_loss 2266.4901428222656\n",
      "train_correct 15\n",
      "train_loss 2518.4615478515625\n",
      "train_correct 15\n",
      "train_loss 2769.4379119873047\n",
      "train_correct 16\n",
      "train_loss 3020.2643127441406\n",
      "train_correct 18\n",
      "train_loss 3271.600860595703\n",
      "train_correct 21\n",
      "train_loss 3523.6705474853516\n",
      "train_correct 22\n",
      "train_loss 3775.1038818359375\n",
      "train_correct 24\n",
      "train_loss 4026.074905395508\n",
      "train_correct 25\n",
      "train_loss 4277.450958251953\n",
      "train_correct 30\n",
      "train_loss 4529.610885620117\n",
      "train_correct 30\n",
      "train_loss 4781.427200317383\n",
      "train_correct 31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tuana\\SLC_PRO\\Model_training.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m save_best_model \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/content/drive/MyDrive/SLC_Project/weights/best.pth\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# train my model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m x \u001b[39m=\u001b[39m train(model \u001b[39m=\u001b[39;49m model, num_epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, optimizer\u001b[39m=\u001b[39;49moptimizer, criterion\u001b[39m=\u001b[39;49mcriterion, train_loader\u001b[39m=\u001b[39;49mtrain_loader, val_loader\u001b[39m=\u001b[39;49mval_loader, save_path\u001b[39m=\u001b[39;49msave_best_model)\n",
      "\u001b[1;32mc:\\Users\\tuana\\SLC_PRO\\Model_training.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# calculate the loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n",
      "File \u001b[1;32mc:\\Users\\tuana\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\tuana\\SLC_PRO\\Model_training.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_2(out)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_3(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_4(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tuana/SLC_PRO/Model_training.ipynb#X25sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(out)\n",
      "File \u001b[1;32mc:\\Users\\tuana\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tuana\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tuana\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tuana\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\tuana\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_best_model = '/content/drive/MyDrive/SLC_Project/weights/best.pth'\n",
    "\n",
    "# train my model\n",
    "x = train(model = model, num_epochs = 10, optimizer=optimizer, criterion=criterion, train_loader=train_loader, val_loader=val_loader, save_path=save_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lkHpJCkM5WcA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self,num_classes=6):\n",
    "        super(VGG16,self).__init__()\n",
    "        self.layer1= nn.Sequential(\n",
    "            nn.Conv2d(3,64, kernel_size = 3, stride =1 , padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64,64,kernel_size=3, stride =1, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride =2 ))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64,128, kernel_size = 3, stride= 1 ,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128,128,kernel_size = 3, stride =1 , padding =1 ),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128,256,kernel_size = 3, stride =1 ,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256,256,kernel_size = 3,stride =1 , padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 =nn.Sequential(\n",
    "            nn.Conv2d(256,256,kernel_size= 3 ,stride = 1,padding =1 ),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size =2, stride= 2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256,512,kernel_size =3,stride =1, padding =1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512,512,kernel_size=3, stride =1 ,padding =1 ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512,512,kernel_size = 3,stride =1, padding =1 ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride =2 ))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512,512,kernel_size= 3, stride =1, padding =1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512,512,kernel_size=3, stride =1, padding =1 ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512,512,kernel_size = 3, stride =1 , padding =1 ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 4096) ,\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(4096,num_classes))\n",
    "    def forward(self,x):\n",
    "        out= self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out= self.layer4(out)\n",
    "        out= self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out= self.layer7(out)\n",
    "        out=self.layer8(out)\n",
    "        out=self.layer9(out)\n",
    "        out=self.layer10(out)\n",
    "        out=self.layer11(out)\n",
    "        out=self.layer12(out)\n",
    "        out=self.layer13(out)\n",
    "        out= out.reshape(out.size(0),-1)\n",
    "        out=self.fc(out)\n",
    "        out= self.fc1(out)\n",
    "        out= self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPDqfN3-DojM"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAkOj60NjerX"
   },
   "source": [
    "## Step 1: Initialize and load weight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tGlEg4T8VFoE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "def inference( model, weight_path, im_path, num_class):\n",
    "  model = VGG16(num_classes = num_class)\n",
    "  #intialize device\n",
    "  # check if GPU is available\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model_statedict = torch.load(weight_path, map_location = 'cpu')\n",
    "  #mapping statedict to model\n",
    "  model.load_state_dict(model_statedict)\n",
    "  #send model to device\n",
    "  model = model.to(device)\n",
    "  # set the model to evaluation mode\n",
    "  model.eval()\n",
    "  image = cv2.imread(image_path)\n",
    "  image_RGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "  # display the image\n",
    "  plt.imshow(image_RGB)\n",
    "  image_pil = Image.open(im_path)\n",
    "  data_transform = transforms.Compose((\n",
    "      transforms.Resize(64),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean = [0.485, 0.456 , 0.406],\n",
    "                          std =[0.229, 0.224, 0.225],)\n",
    "  ))\n",
    "  # Apply the transformations and get the image as a tensor\n",
    "  image_tensor = data_transform(image_pil)\n",
    "  #change 3 dim to 4 dim before inference\n",
    "  im = image_tensor.unsqueeze(0)\n",
    "  # Pass the image through the model\n",
    "  im = im.to(device)\n",
    "  output = model(im)\n",
    "  # apply softmax\n",
    "  output_softmax = torch.softmax(output, dim=1)# because output shape is 1 , 27  which is by column and to calculate column, dim needs to be dim  = 1\n",
    "  # print output\n",
    "  print(output_softmax)\n",
    "  top_k_probs, top_k_classes = torch.topk(output_softmax, k=2)\n",
    "  print(f'top_k_probs : {top_k_probs} /ntop_k_classes : {top_k_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kEiOaYfPjdex"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Gb4LJOHSmJQP"
   },
   "outputs": [],
   "source": [
    "model = VGG16(num_classes = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-kuM6RTumKNr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_3784\\1926428952.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_statedict = torch.load(weight_path, map_location = 'cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG16(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer6): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer7): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer8): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer9): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer10): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer11): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer12): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer13): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intialize device\n",
    "# check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = model.to(device)\n",
    "#define weights path\n",
    "weight_path = 'C:/Users/peter/Github/Chess-voice-recognition/weights/White_detection.pth'\n",
    "#load weight path\n",
    "model_statedict = torch.load(weight_path, map_location = 'cpu')\n",
    "#mapping statedict to model\n",
    "model.load_state_dict(model_statedict)\n",
    "#send model to device\n",
    "model = model.to(device)\n",
    "# set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GGU3HYJnrbFv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a127338100>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "# load the image\n",
    "image = cv2.imread(\"C:/Users/peter/Github/Chess-voice-recognition/test_images/test5.png\")\n",
    "image_RGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image\n",
    "plt.imshow(image_RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMQ9f1c1tpOp"
   },
   "source": [
    "## Preprocessing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yyiAIZaWtndn"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image converted to RGB.\n",
      "Image shape is : torch.Size([1, 3, 224, 224])\n",
      "image size is : (94, 88) \n",
      "output_softmax : tensor([[0.0330, 0.0321, 0.0792, 0.1545, 0.0252, 0.6760]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "top_k_probs : tensor([[0.6760]], grad_fn=<TopkBackward0>)\n",
      "top_k_classes : tensor([[5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Black_rook'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_path = 'C:/Users/peter/Github/Chess-voice-recognition/test_images/test5.png'\n",
    "# Load the image using PIL\n",
    "image_pil = Image.open(im_path)\n",
    "if image_pil.mode != 'RGB':\n",
    "    image = image_pil.convert('RGB')\n",
    "    print(\"Image converted to RGB.\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize with mean and std of ImageNet\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Apply the transformations and get the image as a tensor\n",
    "image_tensor = transform(image)\n",
    "#change 3 dim to 4 dim before inference\n",
    "im = image_tensor.unsqueeze(0)\n",
    "print(f'Image shape is : {im.shape}')\n",
    "print(f'image size is : {image_pil.size} ')\n",
    "# Pass the image through the model\n",
    "im = im.to(device)\n",
    "output = model(im)\n",
    "# apply softmax\n",
    "output_softmax = torch.softmax(output, dim=1)# because output shape is 1 , 27  which is by column and to calculate column, dim needs to be dim  = 1\n",
    "# print output\n",
    "print(f'output_softmax : {output_softmax}')\n",
    "names = ['Black_bishop',\n",
    " 'Black_king',\n",
    " 'Black_knight',\n",
    " 'Black_pawn',\n",
    " 'Black_queen',\n",
    " 'Black_rook']\n",
    "top_k_probs, top_k_classes = torch.topk(output_softmax, k=1)\n",
    "print(f'top_k_probs : {top_k_probs}')\n",
    "print(f'top_k_classes : {top_k_classes}')\n",
    "predicted_class = names[top_k_classes[0]]\n",
    "predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
